# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16pDD0jHnUMZPRXPhzgGxKryHjF3gty9E
"""

import numpy as np
from layers import Layer

# ReLU Activation
class ReLU(Layer):
    def __init__(self):
        self.out = None

    def forward(self, x):
        self.out = np.maximum(0, x)  # max(0, x)
        return self.out

    def backward(self, grad_output):
        grad_input = grad_output * (self.out > 0).astype(float)
        return grad_input

# Sigmoid Activation
class Sigmoid(Layer):
    def __init__(self):
        self.out = None

    def forward(self, x):
        self.out = 1 / (1 + np.exp(-x))
        return self.out

    def backward(self, grad_output):
        grad_input = grad_output * self.out * (1 - self.out)
        return grad_input

# Tanh Activation
class Tanh(Layer):
    def __init__(self):
        self.out = None

    def forward(self, x):
        self.out = np.tanh(x)
        return self.out

    def backward(self, grad_output):
        grad_input = grad_output * (1 - self.out**2)
        return grad_input

# Softmax Activation
class Softmax(Layer):
    def __init__(self):
        self.out = None

    def forward(self, x):
        exp_shifted = np.exp(x - np.max(x, axis=1, keepdims=True))
        self.out = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)
        return self.out

    def backward(self, grad_output):
        # approximate derivative for cross-entropy loss usage
        return grad_output

    """def backward(self, grad_output):
    s = self.out.reshape(-1, 1)
    jac = np.diagflat(s) - np.dot(s, s.T)
    return np.dot(jac, grad_output)"""